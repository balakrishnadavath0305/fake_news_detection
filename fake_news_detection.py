# -*- coding: utf-8 -*-
"""Fake_news_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14uI3nGZFKSuJitskI64UghhVzsoSnDZR

Importing the datasets
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df_true_news=pd.read_csv('/content/drive/MyDrive/True.csv')
df_fake_news=pd.read_csv('/content/drive/MyDrive/Fake.csv')

df_fake_news.head(20)

df_true_news.head(20)

df_fake_news.count()

df_true_news.count()

"""Data Preprocessing"""

def find_missing_vals(df):
    total = len(df)
    for column in df.columns:
        missing = df[column].isna().sum()
        if missing != 0:
            print(f"{column} has: {missing:,} ({(missing/total)*100:.2f}%) missing values.")
        else:
            print(f"{column} has no missing values.")

print("\nMissing Value Summary\n" + "-"*35)
print("\nMissing values per column:")
print(df_fake_news.isnull().sum(axis=0))

print("\nDetailed check:")
find_missing_vals(df_fake_news)

def remove_duplicates(data):
  print("\nCleaning Summary\n{}".format("-"*35))
  size_before=len(data)
  data.drop_duplicates(subset=None,keep='first',inplace=True)
  size_after=len(data)
  print("...removed {} duplicate rows in db data".format(size_before-size_after))
remove_duplicates(df_fake_news)

def find_missing_vals(df):
    total = len(df)
    for column in df.columns:
        missing = df[column].isna().sum()
        if missing != 0:
            print(f"{column} has: {missing:,} ({(missing/total)*100:.2f}%) missing values.")
        else:
            print(f"{column} has no missing values.")

print("\nMissing Value Summary\n" + "-"*35)
print("\nMissing values per column:")
print(df_true_news.isnull().sum(axis=0))

print("\nDetailed check:")
find_missing_vals(df_fake_news)

def remove_duplicates(data):
  print("\nCleaning Summary\n{}".format("-"*35))
  size_before=len(data)
  data.drop_duplicates(subset=None,keep='first',inplace=True)
  size_after=len(data)
  print("...removed {} duplicate rows in db data".format(size_before-size_after))
remove_duplicates(df_true_news)

df_merged=pd.merge(df_fake_news,df_true_news,how='outer')

"""Data Visualization"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.set(style="ticks",color_codes=True)
fig_dims=(20,4.8)
fig,ax=plt.subplots(figsize=fig_dims)
sns.countplot(x=df_merged["subject"], ax=ax)

df_fake_news['label']=0
df_true_news['label']=1

df_train=pd.merge(df_fake_news,df_fake_news,how='outer')

pip install scikit-learn

"""Model Training"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

import string

from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')

def text_process(text):
  no_punctuation=[char for char in text if char not in string.punctuation]
  no_punctuation=''.join(no_punctuation)
  return [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(df_train['title'],df_train['label'],test_size=0.2,random_state=42)

from sklearn.neural_network import MLPClassifier

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.neural_network import MLPClassifier

# Example pipeline
news_classifier = Pipeline([
    ('vectorizer', CountVectorizer()),   # default tokenizer
    ('tfidf', TfidfTransformer()),
    ('classifier', MLPClassifier(
        solver='adam',
        activation='tanh',
        random_state=1,
        max_iter=200,
        early_stopping=True
    ))
])

news_classifier.fit(X_train,y_train)

"""Model Evaluation"""

predicted=news_classifier.predict(X_test)

from sklearn.metrics import classification_report

print(classification_report(predicted,y_test))

"""Model Saving to drive"""

!pip install joblib

import joblib
joblib.dump(news_classifier, 'model.pkl')

from google.colab import drive
drive.mount('/content/drive')

import joblib

# Save the trained model inside Google Drive
joblib.dump(news_classifier, '/content/drive/MyDrive/news_classifier.pkl')

print("âœ… Model saved to Google Drive (MyDrive/news_classifier.pkl)")